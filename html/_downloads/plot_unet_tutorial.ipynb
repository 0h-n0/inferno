{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nUNet Tutorial\n================================\nA tentative tutorial on the usage\nof the unet framework in inferno\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Preface\n--------------\nWe start with some unspectacular multi purpose imports needed for this example\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt \nimport torch\nimport numpy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "should CUDA be used\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "USE_CUDA = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Dataset\n--------------\nFor simplicity we will use a toy dataset where we need to perform\na binary segmentation task.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from inferno.io.box.binary_blobs import get_binary_blob_loaders\n\n# convert labels from long to float as needed by\n# binary cross entropy loss\ndef label_transform(x):\n    return torch.from_numpy(x).float()\n#label_transform = lambda x : torch.from_numpy(x).float()\n\ntrain_loader, test_loader, validate_loader = get_binary_blob_loaders(\n    size=8, # how many images per {train,test,validate}\n    train_batch_size=2,\n    length=256, # <= size of the images\n    gaussian_noise_sigma=1.4, # <= how noise are the images\n    train_label_transform = label_transform,\n    validate_label_transform = label_transform\n)\n\nimage_channels = 1   # <-- number of channels of the image\npred_channels = 1  # <-- number of channels needed for the prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Visualize Dataset\n~~~~~~~~~~~~~~~~~~~~~~\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig = plt.figure()\n\nfor i,(image, target) in enumerate(train_loader):\n    ax = fig.add_subplot(1, 2, 1)\n    ax.imshow(image[0,0,...])\n    ax.set_title('raw data')\n    ax = fig.add_subplot(1, 2, 2)\n    ax.imshow(target[0,...])\n    ax.set_title('ground truth')\n    break\nfig.tight_layout()\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Simple UNet\n----------------------------\nWe start with a very simple predefined\nres block UNet. By default, this UNet uses  ReLUs (in conjunction with batchnorm) as nonlinearities  \nWith :code:`activated=False` we make sure that the last layer\nis not activated since we chain the UNet with a sigmoid\nactivation function.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from inferno.extensions.layers import ResBlockUNet\nfrom inferno.extensions.layers import RemoveSingletonDimension\n\nmodel = torch.nn.Sequential(\n    ResBlockUNet(dim=2, in_channels=image_channels, out_channels=pred_channels,  activated=False),\n    RemoveSingletonDimension(dim=1),\n    torch.nn.Sigmoid()\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "while the model above will work in principal, it has some drawbacks.\nWithin the UNet, the number of features is increased by a multiplicative \nfactor while going down, the so-called gain. The default value for the gain is 2.\nSince we start with only a single channel we could either increase the gain,\nor use a some convolutions to increase the number of channels \nbefore the the UNet.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from inferno.extensions.layers import ConvReLU2D\nmodel_a = torch.nn.Sequential(\n    ConvReLU2D(in_channels=image_channels, out_channels=5, kernel_size=3),\n    ResBlockUNet(dim=2, in_channels=5, out_channels=pred_channels,  activated=False,\n        res_block_kwargs=dict(batchnorm=True,size=2)) ,\n    RemoveSingletonDimension(dim=1)\n    # torch.nn.Sigmoid()\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Training\n----------------------------\nTo train the unet, we use the infernos Trainer class of inferno.\nSince we train many models later on in this example we encapsulate\nthe training in a function (see `sphx_glr_auto_examples_trainer.py` for\nan example dedicated to the trainer itself). \n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from inferno.trainers import Trainer\nfrom inferno.utils.python_utils import ensure_dir\n\ndef train_model(model, loaders, **kwargs):\n\n    trainer = Trainer(model)\n    trainer.build_criterion('BCEWithLogitsLoss')\n    trainer.build_optimizer('Adam', lr=kwargs.get('lr', 0.0001))\n    #trainer.validate_every((kwargs.get('validate_every', 10), 'epochs'))\n    #trainer.save_every((kwargs.get('save_every', 10), 'epochs'))\n    #trainer.save_to_directory(ensure_dir(kwargs.get('save_dir', 'save_dor')))\n    trainer.set_max_num_epochs(kwargs.get('max_num_epochs', 200))\n\n    # bind the loaders\n    trainer.bind_loader('train', loaders[0]) \n    trainer.bind_loader('validate', loaders[1])\n\n    if USE_CUDA:\n        trainer.cuda()\n\n    # do the training\n    trainer.fit()\n\n    return trainer\n\n\ntrainer = train_model(model=model_a, loaders=[train_loader, validate_loader], save_dir='model_a', lr=0.01)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Prediction\n----------------------------\nThe trainer contains the trained model and we can do predictions.\nWe use :code:`unwrap` to convert the results to numpy arrays.\nSince we want to do many prediction we encapsulate the \nthe prediction in a function\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from inferno.utils.torch_utils import unwrap\n\ndef predict(trainer, test_loader,  save_dir=None):\n\n\n    trainer.eval_mode()\n    for image, target in test_loader:\n\n        # transfer image to gpu\n        image = image.cuda() if USE_CUDA else image\n\n        # get batch size from image\n        batch_size = image.size()[0]\n        \n        for b in range(batch_size):\n            prediction = trainer.apply_model(image)\n            prediction = torch.nn.functional.sigmoid(prediction)\n\n            image = unwrap(image,      as_numpy=True, to_cpu=True)\n            prediction = unwrap(prediction, as_numpy=True, to_cpu=True)\n            target = unwrap(target, as_numpy=True, to_cpu=True)\n\n            fig = plt.figure()\n\n            ax = fig.add_subplot(2, 2, 1)\n            ax.imshow(image[b,0,...])\n            ax.set_title('raw data')\n\n            ax = fig.add_subplot(2, 2, 2)\n            ax.imshow(target[b,...])\n            ax.set_title('ground truth')\n\n            ax = fig.add_subplot(2, 2, 4)\n            ax.imshow(prediction[b,...])\n            ax.set_title('prediction')\n\n            fig.tight_layout()\n            plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "do the prediction\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "predict(trainer=trainer, test_loader=test_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Custom UNet\n----------------------------\nOften one needs to have a UNet with custom layers.\nHere we show how to implement such a customized UNet.\nTo this end we derive from :code:`UNetBase`.\nFor the sake of this example we will create \na rather exotic UNet which uses different types\nof convolutions/non-linearities in the different branches\nof the unet\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from inferno.extensions.layers import UNetBase\nfrom inferno.extensions.layers import ConvSELU2D, ConvReLU2D, ConvELU2D, ConvSigmoid2D,Conv2D\n\nclass MySimple2DUnet(UNetBase):\n    def __init__(self, in_channels, out_channels, depth=3, **kwargs):\n        super(MySimple2DUnet, self).__init__(in_channels=in_channels, out_channels=out_channels,\n                                             dim=2, depth=depth, **kwargs)\n\n    def conv_op_factory(self, in_channels, out_channels, part, index):\n\n        if part == 'down':\n            return torch.nn.Sequential(\n                ConvELU2D(in_channels=in_channels,  out_channels=out_channels, kernel_size=3),\n                ConvELU2D(in_channels=out_channels,  out_channels=out_channels, kernel_size=3)\n            ), False\n        elif part == 'bottom':\n            return torch.nn.Sequential(\n                ConvReLU2D(in_channels=in_channels,  out_channels=out_channels, kernel_size=3),\n                ConvReLU2D(in_channels=out_channels,  out_channels=out_channels, kernel_size=3),\n            ), False\n        elif part == 'up':\n            # are we in the very last block?\n            if index + 1 == self.depth:\n                return torch.nn.Sequential(\n                    ConvELU2D(in_channels=in_channels,  out_channels=out_channels, kernel_size=3),\n                    Conv2D(in_channels=out_channels,  out_channels=out_channels, kernel_size=3)\n                ), False\n            else:\n                return torch.nn.Sequential(\n                    ConvELU2D(in_channels=in_channels,   out_channels=out_channels, kernel_size=3),\n                    ConvReLU2D(in_channels=out_channels,  out_channels=out_channels, kernel_size=3)\n                ), False\n        else:\n            raise RuntimeError(\"something is wrong\")\n\n\n\n\n    # this function CAN be implemented, if not, MaxPooling is used by default\n    def downsample_op_factory(self, index):\n        return torch.nn.MaxPool2d(kernel_size=2, stride=2)\n\n    # this function CAN be implemented, if not, Upsampling is used by default\n    def upsample_op_factory(self, index):\n        return torch.nn.Upsample(mode='bilinear', align_corners=False,scale_factor=2)\n\nmodel_b = torch.nn.Sequential(\n    ConvReLU2D(in_channels=image_channels, out_channels=5, kernel_size=3),\n    MySimple2DUnet(in_channels=5, out_channels=pred_channels) ,\n    RemoveSingletonDimension(dim=1)\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "do the training (with the same functions as before)\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "trainer = train_model(model=model_b, loaders=[train_loader, validate_loader], save_dir='model_b', lr=0.001)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "do the training (with the same functions as before)\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "predict(trainer=trainer, test_loader=test_loader)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}