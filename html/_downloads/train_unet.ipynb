{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nTrain UNet Example\n================================\n\nThis example should illustrate how to use the trainer class\nin conjunction with a unet, we use a toy dataset here\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\nfrom inferno.io.box.binary_blobs import get_binary_blob_loaders\nfrom inferno.trainers.basic import Trainer\nfrom inferno.extensions.layers.building_blocks import ResBlock\nfrom inferno.extensions.layers.unet import ResBlockUNet\nfrom inferno.extensions.layers.unet import foo\nfrom inferno.utils.torch_utils import unwrap\nfrom inferno.utils.python_utils import ensure_dir\nimport pylab\n\n\n\n# change directories to your needs\nLOG_DIRECTORY = ensure_dir('log')\nSAVE_DIRECTORY = ensure_dir('save')\nDATASET_DIRECTORY = ensure_dir('dataset')\n\n# should cuda be used\nUSE_CUDA = True\n\n# Build a residual unet where the last layer is not activated\nmodel = nn.Sequential(\n    ResBlock(dim=2, in_channels=1, out_channels=5),\n    ResBlockUNet(dim=2, in_channels=5, out_channels=2,  activated=False) \n)\ntrain_loader, test_loader, validate_loader = get_binary_blob_loaders(\n    train_batch_size=3,\n    length=512, # <= size of the images\n)\n\n# Build trainer\ntrainer = Trainer(model)\ntrainer.build_criterion('CrossEntropyLoss')\ntrainer.build_metric('IOU')\ntrainer.build_optimizer('Adam')\ntrainer.validate_every((10, 'epochs'))\ntrainer.save_every((10, 'epochs'))\ntrainer.save_to_directory(SAVE_DIRECTORY)\ntrainer.set_max_num_epochs(40)\n\n# Bind loaders\ntrainer.bind_loader('train', train_loader) \ntrainer.bind_loader('validate', validate_loader)\n\nif USE_CUDA:\n    trainer.cuda()\n\n# Go!\ntrainer.fit()\n\n\n# predict:\ntrainer.load(best=True)\ntrainer.bind_loader('train', train_loader)\ntrainer.bind_loader('validate', validate_loader)\ntrainer.eval_mode()\n\nif USE_CUDA:\n    trainer.cuda()\n\n# look at an example\nfor x,y in test_loader:\n    if USE_CUDA:\n        x = x.cuda()\n    yy = trainer.apply_model(x)\n    yy = nn.functional.softmax(yy,dim=1)\n    yy = unwrap(yy, as_numpy=True, to_cpu=True)\n    x  = unwrap(x,  as_numpy=True, to_cpu=True)\n    y  = unwrap(y, as_numpy=True, to_cpu=True)\n\n    batch_size = yy.shape[0]\n    for b in range(batch_size):\n\n        fig = pylab.figure()\n        ax1 = fig.add_subplot(1,3,1)\n        ax1.imshow(x[b,0,...])\n        ax2 = fig.add_subplot(1,3,2)\n        ax2.imshow(y[b,...])\n        ax3 = fig.add_subplot(1,3,3)\n        ax3.imshow(yy[b,1,...])\n\n        pylab.show()\n\n    break"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}