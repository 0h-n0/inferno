{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nTrain Side Loss UNet Example\n================================\n\nIn this example a UNet with side supervision\nand auxiliary loss  implemented\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Imports needed for this example\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\nimport torch.nn as nn\nfrom inferno.io.box.binary_blobs import get_binary_blob_loaders\nfrom inferno.trainers.basic import Trainer\n\nfrom inferno.extensions.layers.convolutional import  Conv2D\nfrom inferno.extensions.layers.building_blocks import ResBlock\nfrom inferno.extensions.layers import ResBlockUNet\nfrom inferno.utils.torch_utils import unwrap\nfrom inferno.utils.python_utils import ensure_dir\nimport pylab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To create a UNet with side loss we create a new nn.Module class\nwhich has a ResBlockUNet as member.\nThe ResBlockUNet is configured such that the results of the \nbottom convolution and all the results of the up-stream\nconvolutions are returned as (side)-output.\na 1x1 convolutions is used to give the side outputs\nthe right number of out_channels and UpSampling is\nused to resize all side-outputs to the full resolution\nof the input. These side `side-predictions` are\nreturned by our MySideLossUNet.\nFurthermore, all  `side-predictions` are concatenated\nand feed trough another two residual blocks to make\nthe final prediction.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class MySideLossUNet(nn.Module):\n    def __init__(self, in_channels, out_channels, depth=3):\n        super(MySideLossUNet, self).__init__()\n\n        self.depth = depth\n        self.unet = ResBlockUNet(in_channels=in_channels, out_channels=in_channels*2,\n                                 dim=2, unet_kwargs=dict(depth=depth),\n                                 side_out_parts=['bottom', 'up'])\n\n        # number of out channels\n        self.n_channels_per_output = self.unet.n_channels_per_output\n\n        # 1x1 conv to give the side outs of the unet \n        # the right number of channels \n        # and a Upsampling to give the right shape\n        upscale_factor = 2**self.depth\n        conv_and_scale = []\n        for n_channels in self.n_channels_per_output:\n\n            # conv blocks\n            conv = Conv2D(in_channels=n_channels, out_channels=out_channels, kernel_size=1)\n            if upscale_factor > 1:\n                upsample = nn.Upsample(scale_factor=upscale_factor)\n                conv_and_scale.append(nn.Sequential(conv, upsample))\n            else:\n                conv_and_scale.append(conv)\n\n            upscale_factor //= 2\n\n        self.conv_and_scale = nn.ModuleList(conv_and_scale)\n\n\n        # combined number of channels after concat\n        # concat side output predictions with main output of unet\n        self.n_channels_combined = (self.depth + 1)* out_channels + in_channels*2\n\n        self.final_block = nn.Sequential(\n            ResBlock(dim=2,in_channels=self.n_channels_combined, out_channels=self.n_channels_combined),\n            ResBlock(in_channels=self.n_channels_combined, out_channels=out_channels, \n                    dim=2, activated=False),\n        )\n   \n    def forward(self, input):\n        outs = self.unet(input)\n        assert len(outs) == len(self.n_channels_per_output)\n\n        # convert the unet output into the right number of\n        preds = [None] * len(outs)\n        for i,out in enumerate(outs):\n            preds[i] = self.conv_and_scale[i](out)\n\n        # this is the side output\n        preds =  tuple(preds)\n\n        # concat side output predictions with main output of unet\n        combined = torch.cat(preds + (outs[-1],), 1)\n\n        final_res = self.final_block(combined)\n\n        # return everything\n        return preds + (final_res,)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We use a custom loss functions which applied CrossEntropyLoss\nto all side outputs.\nThe side outputs are weighted in a quadratic fashion and added up\ninto a single value\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class MySideLoss(nn.Module):\n    \"\"\"Wrap a criterion. Collect regularization losses from model and combine with wrapped criterion.\n    \"\"\"\n\n    def __init__(self):\n        super(MySideLoss, self).__init__()\n        self.criterion = nn.CrossEntropyLoss(reduce=True)\n\n        w = 1.0\n        l = None\n\n    def forward(self, predictions, target):\n        w = 1.0\n        l = None\n        for p in predictions:\n            ll = self.criterion(p, target)*w\n            if l is None:\n                l = ll\n            else:\n                l += ll\n            w *= 2\n        return l"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Training boilerplate (see `sphx_glr_auto_examples_trainer.py`)\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "LOG_DIRECTORY = ensure_dir('log')\nSAVE_DIRECTORY = ensure_dir('save')\nDATASET_DIRECTORY = ensure_dir('dataset')\n\n\nUSE_CUDA = True\n\n# Build a residual unet where the last layer is not activated\nsl_unet = MySideLossUNet(in_channels=5, out_channels=2)\n\nmodel = nn.Sequential(\n    ResBlock(dim=2, in_channels=1, out_channels=5),\n    sl_unet\n)\ntrain_loader, test_loader, validate_loader = get_binary_blob_loaders(\n    train_batch_size=3,\n    length=512, # <= size of the images\n    gaussian_noise_sigma=1.5 # <= how noise are the images\n)\n\n# Build trainer\ntrainer = Trainer(model)\ntrainer.build_criterion(MySideLoss())\ntrainer.build_optimizer('Adam')\ntrainer.validate_every((10, 'epochs'))\n#trainer.save_every((10, 'epochs'))\n#trainer.save_to_directory(SAVE_DIRECTORY)\ntrainer.set_max_num_epochs(40)\n\n# Bind loaders\ntrainer \\\n    .bind_loader('train', train_loader)\\\n    .bind_loader('validate', validate_loader)\n\nif USE_CUDA:\n    trainer.cuda()\n\n# Go!\ntrainer.fit()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Predict with the trained network\nand visualize the results\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# predict:\n#trainer.load(best=True)\ntrainer.bind_loader('train', train_loader)\ntrainer.bind_loader('validate', validate_loader)\ntrainer.eval_mode()\n\nif USE_CUDA:\n    trainer.cuda()\n\n# look at an example\nfor img,target in test_loader:\n    if USE_CUDA:\n        img = img.cuda()\n\n    # softmax on each of the prediction\n    preds = trainer.apply_model(img)\n    preds = [nn.functional.softmax(pred,dim=1)        for pred in preds]\n    preds = [unwrap(pred, as_numpy=True, to_cpu=True) for pred in preds]\n    img    = unwrap(img,  as_numpy=True, to_cpu=True)\n    target  = unwrap(target, as_numpy=True, to_cpu=True)\n\n    n_plots = len(preds) + 2\n    batch_size = preds[0].shape[0]\n\n    for b in range(batch_size):\n\n        fig = pylab.figure()\n\n        ax1 = fig.add_subplot(2,4,1)\n        ax1.set_title('image')\n        ax1.imshow(img[b,0,...])\n\n        ax2 = fig.add_subplot(2,4,2)\n        ax2.set_title('ground truth')\n        ax2.imshow(target[b,...])\n    \n        for i,pred in enumerate(preds):\n            axn = fig.add_subplot(2,4, 3+i)\n            axn.imshow(pred[b,1,...])\n\n            if i + 1 < len(preds):\n                axn.set_title('side prediction %d'%i)\n            else:\n                axn.set_title('combined prediction')\n\n        pylab.show()\n\n    break"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}