{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nRegularized MNIST Example\n================================\n\nThis example demonstrates adding and logging arbitrary regularization losses, in this case,\nL2 activity regularization and L1 weight regularization.\n\n- Add a `_losses` dictionary to any module containing loss names and values\n- Use a criterion from `inferno.extensions.criteria.regularized` that will collect and add those losses\n- Call `Trainer.observe_training_and_validation_states` to log the losses as well\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import argparse\nimport sys\n\nimport torch\nimport torch.nn as nn\nfrom torchvision import datasets, transforms\n\nfrom inferno.extensions.layers.reshape import Flatten\nfrom inferno.trainers.basic import Trainer\nfrom inferno.trainers.callbacks.logging.tensorboard import TensorboardLogger\n\n\nclass RegularizedLinear(nn.Linear):\n    def __init__(self, *args, ar_weight=1e-3, l1_weight=1e-3, **kwargs):\n        super(RegularizedLinear, self).__init__(*args, **kwargs)\n        self.ar_weight = ar_weight\n        self.l1_weight = l1_weight\n        self._losses = {}\n\n    def forward(self, input):\n        output = super(RegularizedLinear, self).forward(input)\n        self._losses['activity_regularization'] = (output * output).sum() * self.ar_weight\n        self._losses['l1_weight_regularization'] = torch.abs(self.weight).sum() * self.l1_weight\n        return output\n\n\ndef model_fn():\n    return nn.Sequential(\n        Flatten(),\n        RegularizedLinear(in_features=784, out_features=256),\n        nn.LeakyReLU(),\n        RegularizedLinear(in_features=256, out_features=128),\n        nn.LeakyReLU(),\n        RegularizedLinear(in_features=128, out_features=10)\n    )\n\n\ndef mnist_data_loaders(args):\n    kwargs = {'num_workers': 1, 'pin_memory': True} if args.cuda else {}\n    train_loader = torch.utils.data.DataLoader(\n        datasets.MNIST('./data', train=True, download=True,\n                       transform=transforms.Compose([\n                           transforms.ToTensor(),\n                           transforms.Normalize((0.1307,), (0.3081,))\n                       ])),\n        batch_size=args.batch_size, shuffle=True, **kwargs)\n    test_loader = torch.utils.data.DataLoader(\n        datasets.MNIST('./data', train=False, transform=transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize((0.1307,), (0.3081,))\n        ])),\n        batch_size=args.test_batch_size, shuffle=True, **kwargs)\n    return train_loader, test_loader\n\n\ndef train_model(args):\n    model = model_fn()\n    train_loader, validate_loader = mnist_data_loaders(args)\n\n    # Build trainer\n    trainer = Trainer(model) \\\n        .build_criterion('RegularizedCrossEntropyLoss') \\\n        .build_metric('CategoricalError') \\\n        .build_optimizer('Adam') \\\n        .validate_every((1, 'epochs')) \\\n        .save_every((1, 'epochs')) \\\n        .save_to_directory(args.save_directory) \\\n        .set_max_num_epochs(args.epochs) \\\n        .build_logger(TensorboardLogger(log_scalars_every=(1, 'iteration'),\n                                        log_images_every='never'),\n                      log_directory=args.save_directory)\n\n    # Record regularization losses\n    trainer.logger.observe_training_and_validation_states([\n        'main_loss',\n        'total_regularization_loss',\n        'activity_regularization',\n        'l1_weight_regularization'\n    ])\n\n    # Bind loaders\n    trainer \\\n        .bind_loader('train', train_loader) \\\n        .bind_loader('validate', validate_loader)\n\n    if args.cuda:\n        trainer.cuda()\n\n    # Go!\n    trainer.fit()\n\n\ndef main(argv):\n    # Training settings\n    parser = argparse.ArgumentParser(description='PyTorch MNIST Example')\n    parser.add_argument('--batch-size', type=int, default=64, metavar='N',\n                        help='input batch size for training (default: 64)')\n    parser.add_argument('--save-directory', type=str, default='output/mnist/v1',\n                        help='output directory')\n    parser.add_argument('--test-batch-size', type=int, default=1000, metavar='N',\n                        help='input batch size for testing (default: 1000)')\n    parser.add_argument('--epochs', type=int, default=20, metavar='N',\n                        help='number of epochs to train (default: 20)')\n    parser.add_argument('--no-cuda', action='store_true', default=False,\n                        help='disables CUDA training')\n    args = parser.parse_args(argv)\n    args.cuda = not args.no_cuda and torch.cuda.is_available()\n    train_model(args)\n\n\nif __name__ == '__main__':\n    main(sys.argv[1:])"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}